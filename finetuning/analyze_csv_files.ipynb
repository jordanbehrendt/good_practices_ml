{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from finetuning.model.region_loss import Regional_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss(list_of_df: list[pd.DataFrame], name: str, save_path: str):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    Saves t-test results in a heatmap to the save_path.\n",
    "    \n",
    "    Args:\n",
    "        list_of_df (list[pd.DataFrame]): List of dataframes with prediction and label columns.\n",
    "        name (str): Name of the experiment.\n",
    "        save_path (str): Path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    list_of_df = [df.copy() for df in list_of_df]\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=f\"L{i+1}\")\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns.tolist()\n",
    "        cols_to_drop.extend(['prediction', 'label'])\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: x.item() if isinstance(x, torch.Tensor) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x, list) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    loss_config = ['L1', 'L2', 'L3', 'L4']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        values_matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        # iterate over all loss comparisons\n",
    "        for i in range(len(loss_config)):\n",
    "            signi_buffer = []\n",
    "            value_buffer = []\n",
    "            for j in range(len(loss_config)):\n",
    "                exp1 = loss_config[i]\n",
    "                exp2 = loss_config[j]\n",
    "\n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significance level of 0.05; due to 10-fold validation we have 9:1 ratio of samples\n",
    "                ttest_result = ttest_ind(values1.to_list(), values2.to_list())\n",
    "                t_stat = ttest_result.statistic\n",
    "                p_value = ttest_result.pvalue\n",
    "                degrees_of_freedom = len(values1.to_list()) + len(values2.to_list()) - 2\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        signi_buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        signi_buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    signi_buffer.append(0)\n",
    "                value_str = (f\"M1={values1.mean():.3f}, S1={values1.std():.3f}\\n\"\n",
    "                             f\"M2={values2.mean():.3f}, S2={values2.std():.3f}\\n\"\n",
    "                             f\"t({degrees_of_freedom}) = {t_stat:.3f}, p={p_value:.3f}\")\n",
    "                value_buffer.append(value_str)\n",
    "                \n",
    "            matrix.append(signi_buffer)\n",
    "            values_matrix.append(value_buffer)\n",
    "            \n",
    "        matrix = pd.DataFrame(matrix, index=loss_config, columns=loss_config)\n",
    "        values_matrix = pd.DataFrame(values_matrix, index=loss_config, columns=loss_config)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=values_matrix, fmt='', cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_dataframe(df_data1: pd.DataFrame, df_data2: pd.DataFrame, dataset_names: list[str]):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    Prints the results to the console.\n",
    "    \n",
    "    Args:\n",
    "        df_data1 (pd.DataFrame): Dataframe with metrics of the experiment, assuming structure as created by calculate_metrics.\n",
    "        df_data2 (pd.DataFrame): Dataframe with metrics of the other experiment, assuming structure as created by calculate_metrics.\n",
    "        dataset_names (list[str]): Names of the datasets.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cols_to_drop = df_data1.filter(like='text', axis=1).columns.tolist()\n",
    "    cols_to_drop.extend(['prediction', 'label'])\n",
    "    df_data1 = df_data1.drop(columns=cols_to_drop)\n",
    "    df_data2 = df_data2.drop(columns=cols_to_drop)\n",
    "    df_data1.columns = df_data1.columns.str.split().str[-2:].str.join(\" \")\n",
    "    df_data2.columns = df_data2.columns.str.split().str[-2:].str.join(\" \")\n",
    "    metrics = df_data1.columns[:-1]\n",
    "        \n",
    "    for metric in metrics:\n",
    "        print(f\"Metric: {metric}\")\n",
    "        data1 = df_data1[metric]\n",
    "        data2 = df_data2[metric]\n",
    "        t_stat, p_value = ttest_ind(data1.to_list(), data2.to_list())\n",
    "        degrees_of_freedom = len(data1.to_list()) + len(data2.to_list()) - 2\n",
    "        if p_value < 0.05:\n",
    "            if data1.mean() > data2.mean():\n",
    "                print(f\"{dataset_names[0]} is significantly better than {dataset_names[1]}\")\n",
    "            else:\n",
    "                print(f\"{dataset_names[1]} is significantly better than {dataset_names[0]}\")\n",
    "        else:\n",
    "            print(f\"No significant difference between {dataset_names[0]} and {dataset_names[1]}\")\n",
    "        print(f\"data1 = {dataset_names[0]}, data2={dataset_names[1]}\")\n",
    "        print(f\"M1={data1.mean():.3f}, S1={data1.std():.3f}\\n M2={data2.mean():.3f}, S2={data2.std():.3f}\\n t({degrees_of_freedom}) = {t_stat:.3f}, p={p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(df: pd.DataFrame, data_type: str, REPO_PATH: str):\n",
    "    \"\"\"\n",
    "    Calculate all intresting metrics for the given data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the model predictions and labels.\n",
    "        data_type (str): The type of data (validation or test).\n",
    "        REPO_PATH (str): The path to the repository.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    country_list = f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv'\n",
    "    country_list = pd.read_csv(country_list)\n",
    "    metrics_calculator = Regional_Loss(country_list=country_list)\n",
    "    metrics_calculator.device = 'cpu'\n",
    "    # Convert the 'Output' column to a list of tensors\n",
    "    df['Output'] = df['Output'].apply(lambda x: torch.tensor(x))\n",
    "    if data_type == 'validation':\n",
    "        dfs = np.array_split(df, 10)\n",
    "        for df in dfs:\n",
    "            # Stack the list of tensors into a single tensor\n",
    "            outputs = torch.stack(df['Output'].tolist())\n",
    "            # calculate the metrics\n",
    "            c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "            c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "            r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "            r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "            m_prec, m_rec, m_f1,_,_ = metrics_calculator.calculate_mixed_metrics(outputs, df['Label'])\n",
    "            ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "            ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "            metrics = {\n",
    "                'country_accuracy': [c_ac],\n",
    "                'country_precision': [c_prec.mean()],\n",
    "                'country_recall': [c_rec.mean()],\n",
    "                'country_f1': [c_f1.mean()],\n",
    "                'region_accuracy': [r_ac],\n",
    "                'region_precision': [r_prec.mean()],\n",
    "                'region_recall': [r_rec.mean()],\n",
    "                'region_f1': [r_f1.mean()],\n",
    "                'mixed_precision': [m_prec.mean()],\n",
    "                'mixed_recall': [m_rec.mean()],\n",
    "                'mixed_f1': [m_f1.mean()],\n",
    "                'ignored_classes': [ignored_class],\n",
    "                'ignored_regions': [ignored_region],\n",
    "                'prediction': [df['Prediction']],\n",
    "                'label': [df['Label']]\n",
    "            }\n",
    "            if 'all_metrics' in locals():\n",
    "                all_metrics = pd.concat([all_metrics, pd.DataFrame(metrics)])\n",
    "            else:\n",
    "                all_metrics = pd.DataFrame(metrics)\n",
    "    else:\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        outputs = torch.stack(df['Output'].tolist())\n",
    "        # calculate the metrics\n",
    "        c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "        c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "        r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "        r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "        m_prec, m_rec, m_f1 = metrics_calculator.calculate_mixed_metrics(outputs, df['Label'])\n",
    "        ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "        ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "        metrics = {\n",
    "            'country_accuracy': [c_ac],\n",
    "            'country_precision': [c_prec.mean()],\n",
    "            'country_recall': [c_rec.mean()],\n",
    "            'country_f1': [c_f1.mean()],\n",
    "            'region_accuracy': [r_ac],\n",
    "            'region_precision': [r_prec.mean()],\n",
    "            'region_recall': [r_rec.mean()],\n",
    "            'region_f1': [r_f1.mean()],\n",
    "            'mixed_precision': [m_prec.mean()],\n",
    "            'mixed_recall': [m_rec.mean()],\n",
    "            'mixed_f1': [m_f1.mean()],\n",
    "            'ignored_classes': [ignored_class],\n",
    "            'ignored_regions': [ignored_region],\n",
    "            'prediction': [df['Prediction']],\n",
    "            'label': [df['Label']]\n",
    "        }\n",
    "        all_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_dir(log_dir: str, REPO_PATH: str):\n",
    "    \"\"\"\n",
    "    Read the csv files in the log directory and calculate the metrics for the data.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): The path to the log directory.\n",
    "        REPO_PATH (str): The path to the repository.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: The list of dataframes containing the metrics for each experiment configuration.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the dataframes\n",
    "    validation_dfs = []\n",
    "    test_dfs = []\n",
    "    zero_shot_dfs = []\n",
    "\n",
    "    # Iterate over the folders in the log directory\n",
    "    for folder in sorted(os.listdir(log_dir)):\n",
    "        folder_path = os.path.join(log_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # calculate the metrics for all seeds in the folder\n",
    "            log_files = glob.glob(folder_path + \"/*\")\n",
    "            validation_buffer = []\n",
    "            test_buffer = []\n",
    "            zero_shot_buffer = []\n",
    "            for file_path in log_files:\n",
    "                if '.csv' not in file_path:\n",
    "                    continue\n",
    "                df = pd.read_csv(file_path,converters={\"Output\": ast.literal_eval})\n",
    "\n",
    "                # Split the data into validation and test data\n",
    "                #if 'validation' in file_path:\n",
    "                #    df = calculate_metrics(df, 'validation', REPO_PATH=REPO_PATH)\n",
    "                #    validation_buffer.append(df)\n",
    "                if 'zero' in file_path:\n",
    "                    df = calculate_metrics(df, 'zero', REPO_PATH=REPO_PATH)\n",
    "                    zero_shot_buffer.append(df)\n",
    "                elif 'test' in file_path:\n",
    "                    df = calculate_metrics(df, 'test', REPO_PATH=REPO_PATH)\n",
    "                    test_buffer.append(df)\n",
    "            #validation_dfs.append(pd.concat(validation_buffer))\n",
    "            validation_dfs=[] # currently not used to save computation time\n",
    "            test_dfs.append(pd.concat(test_buffer))\n",
    "            zero_shot_dfs.append(pd.concat(zero_shot_buffer))\n",
    "    return validation_dfs, test_dfs, zero_shot_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_experiments(list_of_df: list[pd.DataFrame], name: str, save_path: str, loss_number: int=0, dataset_names: list[str]=None, metric_names:list[str]=None, legend_out_of_plot: bool=False):\n",
    "    \"\"\"\n",
    "    Genreates box plots for all metrics contained in the dataframes.\n",
    "    Compares these metrics for each dataframe in the list.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_df (list[pd.DataFrame]): A list of dataframes containing the metrics.\n",
    "    name (str): The name of the experiment.\n",
    "    save_path (str): The path to save the plot.\n",
    "    loss_number (int): The loss configuration to use for the comparison.\n",
    "    dataset_names (list[str]): The names of the datasets to compare.\n",
    "    metric_names (list[str]): The names of the metrics to compare.\n",
    "    legend_out_of_plot (bool): Whether to place the legend outside of the plot.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The concatenated dataframe containing the metrics of the experiments shown in the boxplot.\n",
    "    \"\"\"\n",
    "    dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly\\n Balanced':3, 'Mixed Weakly\\n Balanced':4}\n",
    "    if dataset_names is not None:\n",
    "        indices = [dataset_to_indices[name] for name in dataset_names]\n",
    "        list_of_df = [list_of_df[i] for i in indices]\n",
    "\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    list_of_df = [df[loss_number].copy() for df in list_of_df]\n",
    "\n",
    "    cols_to_use = metric_names.copy()\n",
    "    cols_to_use.append('Experiment')\n",
    "\n",
    "    for i in range(len(list_of_df)):\n",
    "\n",
    "\n",
    "        keys = list(dataset_to_indices.keys())\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=keys[indices[i]])\n",
    "        list_of_df[i] = list_of_df[i][cols_to_use]\n",
    "        #cols_to_drop = list_of_df[i].filter(like='text', axis=1).columns\n",
    "        #list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "\n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "        list_of_df[i] = list_of_df[i].rename(columns={'country_accuracy': 'Accuracy', 'country_precision': 'Precision', 'country_recall': 'Recall', 'country_f1': 'F1', 'region_accuracy': 'Accuracy', 'region_precision': 'Precision', 'region_recall': 'Recall', 'region_f1': 'F1', 'mixed_precision': 'Precision', 'mixed_recall': 'Recall', 'mixed_f1': 'F1'})\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x,list) else x) \n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x.item()) if isinstance(x,torch.Tensor) else x) \n",
    "\n",
    "    ax = sns.boxplot(\n",
    "        x=\"Metric\", y=\"Value\", hue=\"Experiment\", data=meltdf, showfliers=False\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    if legend_out_of_plot:\n",
    "        lgd = plt.legend(loc='upper left', fontsize='small', borderaxespad=0.0, bbox_to_anchor=(1, 1))\n",
    "    else:\n",
    "        lgd = plt.legend(loc='upper right', fontsize='small', borderaxespad=0.0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.savefig(\n",
    "        save_path + f\"{name}-boxplot.png\",\n",
    "        bbox_extra_artists=(lgd,),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return condf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_loss(list_of_df: list[list[pd.DataFrame]], name:str, save_path:str, dataset_names:list[str]=None, legend_out_of_plot:bool=False):\n",
    "    \"\"\"\n",
    "    Genreates box plots for all metrics contained in the dataframes.\n",
    "    Compares the mixed F1 for the diffrent loss configurations for each experiment.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_df (list[list[pd.DataFrame]]): A list of the diffrent experiments containing a list with the results for the diffrent loss configurations.\n",
    "    name (str): The name of the experiment.\n",
    "    save_path (str): The path to save the plot.\n",
    "    dataset_names (list[str]): The names of the datasets to compare.\n",
    "    legend_out_of_plot (bool): Whether to place the legend outside of the plot.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated dataframe containing all data with a coloumn tagging the used Loss.\n",
    "    \"\"\"\n",
    "    dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly\\n Balanced':3, 'Mixed Weakly\\n Balanced':4}\n",
    "    keys = list(dataset_to_indices.keys())\n",
    "    if dataset_names is not None:\n",
    "        indices = [dataset_to_indices[name] for name in dataset_names]\n",
    "        list_of_df = [list_of_df[i] for i in indices]\n",
    "\n",
    "    # select the mixed F1 for each loss configuration\n",
    "    for i in range(len(list_of_df)):\n",
    "        buffer_df = pd.DataFrame()\n",
    "        \n",
    "        for j,loss in enumerate([\"$L_0$\",\"$L_{25}$\", \"$L_{50}$\", \"$L_{DYN}$\"]):\n",
    "            buffer_df[loss] = list_of_df[i][j]['mixed_f1']\n",
    "        # add the experiment name as an indicator for the boxplot\n",
    "        buffer_df=buffer_df.assign(Experiment=keys[indices[i]])\n",
    "        list_of_df[i] = buffer_df\n",
    "    # concatenate the dataframes\n",
    "    condf = pd.concat(list_of_df)\n",
    "    # melt the dataframe for the boxplot\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Loss\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x,list) else x) \n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x.item()) if isinstance(x,torch.Tensor) else x) \n",
    "    meltdf = meltdf.rename(columns={'Value': 'Mixed F1'})\n",
    "    ax = sns.boxplot(\n",
    "        x=\"Experiment\", y=\"Mixed F1\", hue=\"Loss\", data=meltdf, showfliers=False, width=0.75\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    if legend_out_of_plot:\n",
    "        lgd = plt.legend(loc='upper left', fontsize='small', borderaxespad=0.0, bbox_to_anchor=(1, 1))\n",
    "    else:\n",
    "        lgd = plt.legend(loc='upper right', fontsize='small', borderaxespad=0.0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.savefig(\n",
    "        save_path + f\"{name}-boxplot.png\",\n",
    "        bbox_extra_artists=(lgd,),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return condf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = 'repo/path/'\n",
    "\n",
    "# directory of all experiments\n",
    "experimient_dir= 'experiment/data/dir/'\n",
    "# create lists that contain the dataframes of the different experiments\n",
    "# First axis contains the different dataset configurations\n",
    "# Second axis contains the different Loss configurations\n",
    "# Third axis contains the DataFrame of the different seeds\n",
    "validation_sets = []\n",
    "test_sets = []\n",
    "zeros_shot_datasets = []\n",
    "\n",
    "\n",
    "for folder in sorted(os.listdir(experimient_dir)):\n",
    "    log_dir = os.path.join(experimient_dir, folder)\n",
    "    if os.path.isdir(log_dir):\n",
    "        save_path = log_dir + '/results/'\n",
    "        # Call the event_to_df function with the log directory \n",
    "        val, test, zero= read_csv_from_dir(log_dir,REPO_PATH)\n",
    "        #validation_sets.append(val)\n",
    "        test_sets.append(test)\n",
    "        zeros_shot_datasets.append(zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(val) for val in test_sets])\n",
    "print(len(test_sets))\n",
    "sorted(os.listdir(experimient_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Comparison\n",
    "Compare the different loss configurations within each experiment using independent t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "save_path = 'save/path/'\n",
    "for i, experiment in enumerate(test_sets):\n",
    "    name = dataset_names[i]\n",
    "    compare_loss(experiment, name, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Comparison\n",
    "Compare the different experiments using the loss configurations loss_number (in this case 2 for the L_{50} loss) using independent t-tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_number = 2\n",
    "\n",
    "l_test_sets = [sublist[loss_number] for sublist in test_sets]\n",
    "l_geo_strongly_balanced = l_test_sets[0]\n",
    "l_geo_weakly_balanced = l_test_sets[2]\n",
    "l_geo_mixed_strongly_balanced = l_test_sets[3]\n",
    "l_geo_mixed_weakly_balanced = l_test_sets[4]\n",
    "compare_dataframe(l_geo_strongly_balanced, l_geo_mixed_strongly_balanced, ['Strongly Balanced', 'Mixed Strongly Balanced'])\n",
    "compare_dataframe(l_geo_weakly_balanced, l_geo_mixed_weakly_balanced, ['Weakly Balanced', 'Mixed Weakly Balanced'])\n",
    "compare_dataframe(l_geo_strongly_balanced, l_geo_weakly_balanced, ['Strongly Balanced', 'Weakly Balanced'])\n",
    "compare_dataframe(l_geo_mixed_strongly_balanced, l_geo_mixed_weakly_balanced, ['Mixed Strongly Balanced', 'Mixed Weakly Balanced'])\n",
    "compare_dataframe(l_geo_strongly_balanced, l_geo_mixed_strongly_balanced, ['Strongly Balanced', 'Mixed Weakly Balanced'])\n",
    "compare_dataframe(l_geo_mixed_strongly_balanced, l_geo_mixed_weakly_balanced, ['Mixed Strongly Balanced', 'Weakly Balanced'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot Experiments\n",
    "Filter and boxplot interesting visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_sets_with_reg[df.filter(like='region') for df in sub_array] for sub_array in test_sets]\n",
    "test_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in test_sets]\n",
    "test_sets_mixed = [[df.filter(like='mixed') for df in sub_array] for sub_array in test_sets]\n",
    "\n",
    "zero_shot_sets_with_region = [[df.filter(like='region') for df in sub_array] for sub_array in zeros_shot_datasets]\n",
    "zero_shot_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in zeros_shot_datasets]\n",
    "zero_shot_sets_mixed = [[df.filter(like='mixed') for df in sub_array] for sub_array in zeros_shot_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Weakly Balanced', 'Mixed Strongly\\n Balanced', 'Mixed Weakly\\n Balanced']\n",
    "box_plot_loss(test_sets_mixed, 'Loss Comparison', save_path, dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "#box_plot_experiments(validation_sets_with_region, 'Validation Region', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "#box_plot_experiments(validation_sets_with_country, 'Validation Country', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "\n",
    "#box_plot_experiments(test_sets_with_region, 'Regions on Test-Set', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(test_sets_with_country, 'Countries on Test-Set', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "box_plot_experiments(test_sets_mixed, 'Mixed on Test-Set', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['mixed_precision', 'mixed_recall', 'mixed_f1'], legend_out_of_plot=True)\n",
    "\n",
    "box_plot_experiments(zero_shot_sets_with_region, 'Zero Shot Regions', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(zero_shot_sets_with_country, 'Zero Shot Countries', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "box_plot_experiments(zero_shot_sets_mixed, 'Zero Shot Mixed', save_path, loss_number=loss_number, dataset_names=dataset_names, metric_names=['mixed_precision', 'mixed_recall', 'mixed_f1'], legend_out_of_plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate result tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(os.listdir(experimient_dir)))\n",
    "\n",
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}').drop(columns=['prediction', 'label', 'ignored_classes', 'ignored_regions','mixed_precision', 'mixed_recall', 'mixed_f1']))\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(os.listdir(experimient_dir)))\n",
    "\n",
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}')[['Experiment', 'mixed_precision', 'mixed_recall', 'mixed_f1']])\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}')[['ignored_classes', 'ignored_regions', 'Experiment']])\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "box_plot_experiments(test_sets, 'Ignored Classes on Test Set', save_path, loss_number=loss_number, metric_names=['ignored_classes'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "box_plot_experiments(test_sets, 'Ignored Regions on Test Set', save_path, loss_number=loss_number, metric_names=['ignored_regions'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusions Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def create_and_save_confusion_matrices(REPO_PATH: str, SAVE_FIGURES_PATH: str, true_countries: list[int], predicted_countries: list[int], regional_ordering_index: list[int], normalize:bool=False, selected_countries: list[int]=None):\n",
    "    \"\"\"\n",
    "    Create and save confusion matrices for countries and regions.\n",
    "\n",
    "    Args:\n",
    "        REPO_PATH (str): path to repo folder.\n",
    "        SAVE_FIGURES_PATH (str): path to save the confusion matrices.\n",
    "        true_countries (list[str]): list of true country labels.\n",
    "        predicted_countries (list[int]): list of predicted country index.\n",
    "        regional_ordering_index (list[int]): list of regional ordering index.\n",
    "        normalize (bool): whether to normalize the confusion matrix.\n",
    "        selected_countries (list[int]): list of selected countries to include in the confusion matrix.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load\n",
    "     country list and regional ordering index\n",
    "    country_list = pd.read_csv(f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv')\n",
    "\n",
    "    # constant for classes\n",
    "    classes = country_list['Country']\n",
    "    np_classes = np.array(classes)\n",
    "    country_dict = {country: index for index, country in enumerate(country_list[\"Country\"])}\n",
    "\n",
    "    # Build country confusion matrices\n",
    "    #if normalize:\n",
    "    #    cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=range(0, 211), normalize='true')\n",
    "    #else:\n",
    "    #    cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=range(0, 211))\n",
    "    \n",
    "    # Get the unique classes from the 'prediction' and 'label' columns\n",
    "    filtered_classes = list(set([*true_countries, *predicted_countries]))\n",
    "    class_indices = [country_dict[country] for country in filtered_classes]\n",
    "    regional_ordering_index = [x for x in regional_ordering_index if x in class_indices]\n",
    "    true_countries_indices = [country_dict[country] for country in true_countries]\n",
    "    predicted_countries_indices = [country_dict[country] for country in predicted_countries]\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=classes, normalize='true')\n",
    "    else:\n",
    "        cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=classes)\n",
    "    ordered_index = np.argsort(-cf_matrix.diagonal())\n",
    "    ordered_index = [x for x in ordered_index if x in class_indices]\n",
    "    ordered_matrix = cf_matrix[ordered_index][:, ordered_index]\n",
    "    regionally_ordered_matrix = cf_matrix[regional_ordering_index][:,regional_ordering_index]\n",
    "    ordered_classes = np_classes[ordered_index]\n",
    "    regionally_ordered_classes = np_classes[regional_ordering_index]\n",
    "\n",
    "    df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "    ordered_df_cm = pd.DataFrame(\n",
    "        ordered_matrix, index=ordered_classes, columns=ordered_classes)\n",
    "    regionally_ordered_df_cm = pd.DataFrame(\n",
    "        regionally_ordered_matrix, index=regionally_ordered_classes, columns=regionally_ordered_classes)\n",
    "\n",
    "    if selected_countries is not None:\n",
    "        df_cm = df_cm.loc[selected_countries, selected_countries]\n",
    "        ordered_df_cm = ordered_df_cm.loc[selected_countries, selected_countries]\n",
    "        regionally_ordered_df_cm = regionally_ordered_df_cm.iloc[selected_countries, selected_countries]\n",
    "\n",
    "    #Create region labels\n",
    "    np_regions = np.sort(np.array(list(set(country_list['Intermediate Region Name']))))\n",
    "    true_regions = []\n",
    "    true_regions_indices = []\n",
    "    predicted_regions = []\n",
    "    predicted_regions_indices = []\n",
    "    for i in range(0, len(true_countries)):\n",
    "        true_country_index = country_dict[true_countries[i]]\n",
    "        predicted_country_index = country_dict[predicted_countries[i]]\n",
    "        true_regions.append(country_list.iloc[true_country_index][\"Intermediate Region Name\"])\n",
    "        predicted_regions.append(country_list.iloc[predicted_country_index][\"Intermediate Region Name\"])\n",
    "        true_regions_indices.append(ast.literal_eval(country_list.iloc[true_countries_indices[i]][\"One Hot Region\"]).index(1))\n",
    "        predicted_regions_indices.append(ast.literal_eval(country_list.iloc[predicted_countries_indices[i]][\"One Hot Region\"]).index(1))\n",
    "\n",
    "    region_indices = list(set([*true_regions_indices, *predicted_regions_indices]))\n",
    "\n",
    "    # Build region confusion matrices\n",
    "    #if normalize:\n",
    "    #    regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=range(0, len(np_regions)), normalize='true')\n",
    "    #else:\n",
    "    #    regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=range(0, len(np_regions)))\n",
    "    if normalize:\n",
    "        regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=np_regions, normalize='true')\n",
    "    else:\n",
    "        regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=np_regions)\n",
    "    regions_ordered_index = np.argsort(-regions_cf_matrix.diagonal())\n",
    "    regions_ordered_index = [x for x in regions_ordered_index if x in region_indices]\n",
    "    regions_ordered_matrix = regions_cf_matrix[regions_ordered_index][:,regions_ordered_index]\n",
    "    ordered_regions = np_regions[regions_ordered_index]\n",
    "\n",
    "    regions_df_cm = pd.DataFrame(regions_cf_matrix, index=np_regions, columns=np_regions)\n",
    "    regions_ordered_df_cm = pd.DataFrame(regions_ordered_matrix, index=ordered_regions, columns=ordered_regions)\n",
    "\n",
    "    # Save confusion matrices\n",
    "    if normalize:\n",
    "        if not os.path.exists(f'{SAVE_FIGURES_PATH}/normalized'):\n",
    "            os.makedirs(f'{SAVE_FIGURES_PATH}/normalized')\n",
    "    else:\n",
    "        if not os.path.exists(f'{SAVE_FIGURES_PATH}'):\n",
    "            os.makedirs(f'{SAVE_FIGURES_PATH}')       \n",
    "\n",
    "    fig_1, ax_1 = plt.subplots(figsize=(120, 90))\n",
    "    sns.set(font_scale=8)\n",
    "    ax_1 = sns.heatmap(df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1, annot=True, annot_kws={\"fontsize\": 8})\n",
    "    #ax_1.tick_params(axis='both', labelsize=15)\n",
    "    ax_1.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_1.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/simple_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_1.figure.savefig(f'{SAVE_FIGURES_PATH}/simple_confusion_matrix.png')\n",
    "    fig_2, ax_2 = plt.subplots(figsize=(120, 90))\n",
    "    ax_2 = sns.heatmap(ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels=1,yticklabels=1, annot=True, annot_kws={\"fontsize\": 8})\n",
    "    #ax_2.tick_params(axis='both', labelsize=15)\n",
    "    ax_2.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_2.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_2.figure.savefig(f'{SAVE_FIGURES_PATH}/ordered_confusion_matrix.png')\n",
    "    fig_3, ax_3 = plt.subplots(figsize=(120, 90))\n",
    "    ax_3 = sns.heatmap(regionally_ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels=1,yticklabels=1, annot=True, annot_kws={\"fontsize\": 8})\n",
    "    #ax_3.tick_params(axis='both', labelsize=15)\n",
    "    ax_3.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_3.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regionally_ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_3.figure.savefig(f'{SAVE_FIGURES_PATH}/regionally_ordered_confusion_matrix.png')\n",
    "    fig_4, ax_4 = plt.subplots(figsize=(120, 90))\n",
    "    ax_4 = sns.heatmap(regions_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1, annot=True, annot_kws={\"fontsize\": 8})\n",
    "    #ax_4.tick_params(axis='both', labelsize=50)\n",
    "    ax_4.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_4.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regions_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_4.figure.savefig(f'{SAVE_FIGURES_PATH}/regions_confusion_matrix.png')\n",
    "    fig_5, ax_5 = plt.subplots(figsize=(120, 90))\n",
    "    ax_5 = sns.heatmap(regions_ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1, annot=True, annot_kws={\"fontsize\": 8})\n",
    "    #ax_5.tick_params(axis='both', labelsize=50)\n",
    "    ax_5.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_5.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regions_ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_5.figure.savefig(f'{SAVE_FIGURES_PATH}/regions_ordered_confusion_matrix.png')\n",
    "    fig_1.clf()\n",
    "    fig_2.clf()\n",
    "    fig_3.clf()\n",
    "    fig_4.clf()\n",
    "    fig_5.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(l_test_sets):\n",
    "    matrix_save_path = save_path + dataset_names[i] + '/'\n",
    "    true_countries = pd.concat(dataset['label'].tolist())\n",
    "    print(true_countries)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly Balanced':3, 'Mixed Weakly Balanced':4}\n",
    "country_list = pd.read_csv(f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv')\n",
    "country_dict = {country: index for index, country in enumerate(country_list[\"Country\"])}\n",
    "\n",
    "# Northern Europe and Western Asia\n",
    "selected_region_indices = [13,21]\n",
    "region_indices = country_list.groupby('Intermediate Region Name')['Country'].apply(lambda x: list(x.index)).to_list()\n",
    "region_country_names = country_list.groupby('Intermediate Region Name')['Country'].to_list()\n",
    "region_names = np.sort(country_list['Intermediate Region Name'].unique())\n",
    "region_country_names = [region_country_names[x] for x in selected_region_indices]\n",
    "region_names = region_names[selected_region_indices]\n",
    "regional_ordering_index = [x for xs in region_indices for x in xs]\n",
    "\n",
    "\n",
    "for i, dataset in enumerate(l_test_sets):\n",
    "\n",
    "    matrix_save_path = save_path + dataset_names[i] + '/'\n",
    "    true_countries = pd.concat(dataset['label'].tolist()).values\n",
    "    predicted_countries = pd.concat(dataset['prediction'].tolist()).values\n",
    "    #true_countries = true_countries.map(country_dict)\n",
    "    #predicted_countries = predicted_countries.map(country_dict)\n",
    "    #create_and_save_confusion_matrices(REPO_PATH=REPO_PATH, SAVE_FIGURES_PATH=matrix_save_path, true_countries=true_countries, predicted_countries=predicted_countries, regional_ordering_index=regional_ordering_index, normalize=True)\n",
    "    for region_name, region_idx in zip(region_names, region_indices):\n",
    "        region_save_path = f\"{matrix_save_path}{region_name}/\"\n",
    "        create_and_save_confusion_matrices(REPO_PATH=REPO_PATH, SAVE_FIGURES_PATH=region_save_path, true_countries=true_countries, predicted_countries=predicted_countries, regional_ordering_index=regional_ordering_index, normalize=True, selected_countries=region_country_names)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
