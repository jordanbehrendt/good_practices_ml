{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.analyzation_tools import read_event_for_different_seeds, box_plot_experiments, event_to_df\n",
    "from utils.analyzation_tools import corrected_repeated_kFold_cv_test as cv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory of all experiments\n",
    "experimient_dir= ''\n",
    "# create lists that contain the dataframes of the different experiments\n",
    "# First axis contains the different dataset configurations\n",
    "# Second axis contains the different Loss configurations\n",
    "# Third axis contains the DataFrame of the different seeds\n",
    "region_val_datasets = []\n",
    "country_val_datasets = []\n",
    "region_test_datasets = []\n",
    "country_test_datasets = []\n",
    "other_coloumns_list = []\n",
    "\n",
    "for folder in sorted(os.listdir(experimient_dir)):\n",
    "    log_dir = os.path.join(experimient_dir, folder)\n",
    "    if os.path.isdir(log_dir):\n",
    "        save_path = log_dir + '/results/'\n",
    "        # Call the event_to_df function with the log directory \n",
    "        rv, cv, rt, ct, o = event_to_df(log_dir)\n",
    "        region_val_datasets.append(rv)\n",
    "        country_val_datasets.append(cv)\n",
    "        region_test_datasets.append(rt)\n",
    "        country_test_datasets.append(ct)\n",
    "        other_coloumns_list.append(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(region_val_datasets)\n",
    "sorted(os.listdir(experimient_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows that for L2 and L3 in some seeds the region precision/recall/f1 are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "read_event_for_different_seeds(path)['Epoch Validation List of Ignored Regions/text_summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(list_of_df, name, save_path):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    list_of_df = [df.copy() for df in list_of_df]\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=f\"L{i+1}\")\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if type(x) == list else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    loss_config = ['L1', 'L2', 'L3', 'L4']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        for i in range(len(loss_config)):\n",
    "            buffer = []\n",
    "            for j in range(len(loss_config)):\n",
    "                exp1 = loss_config[i]\n",
    "                exp2 = loss_config[j]\n",
    "                \n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significnce level of 0.05; due to 10 flod validation we have 9:1 ration of samples\n",
    "                _,_,_, p_value = cv_test(values1.to_list(), values2.to_list(), 9, 1, 0.05)\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    buffer.append(0)\n",
    "            matrix.append(buffer)\n",
    "        matrix = pd.DataFrame(matrix, index=loss_config, columns=loss_config)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=True, cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_datasets_for_l1(list_of_df, name, save_path):\n",
    "    list_of_df = [df[3].copy() for df in list_of_df]\n",
    "    dataset_config = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=dataset_config[i])\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if type(x) == list else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    \n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        for i in range(len(dataset_config)):\n",
    "            buffer = []\n",
    "            for j in range(len(dataset_config)):\n",
    "                exp1 = dataset_config[i]\n",
    "                exp2 = dataset_config[j]\n",
    "                \n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significnce level of 0.05; due to 10 flod validation we have 9:1 ration of samples\n",
    "                _,_,_, p_value = cv_test(values1.to_list(), values2.to_list(), 9, 1, 0.05)\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    buffer.append(0)\n",
    "            matrix.append(buffer)\n",
    "        matrix = pd.DataFrame(matrix, index=dataset_config, columns=dataset_config)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=True, cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_path = 'path/to/save/results/'\n",
    "# Define the directory path\n",
    "directory = 'path/to/data/'\n",
    "dataset_config = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Unbalanced', 'Mixed Weakly Balanced']\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "path = save_path + f\"/region_val/loss_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for i in range(len(region_val_datasets)):\n",
    "    compare_loss(region_val_datasets[i], f'region_val_{dataset_config[i]}', path)\n",
    "\n",
    "path = save_path + f\"/region_val/dataset_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "compare_datasets_for_l1(region_val_datasets, 'region_val', path)\n",
    "\n",
    "path = save_path + f\"/region_test/loss_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for i in range(len(region_test_datasets)):\n",
    "    compare_loss(region_test_datasets[i], f'region_test_{dataset_config[i]}', path)\n",
    "\n",
    "path = save_path + f\"/region_test/dataset_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "compare_datasets_for_l1(region_test_datasets, 'region_test', path)\n",
    "\n",
    "path = save_path + f\"/country_val/loss_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for i in range(len(country_val_datasets)):\n",
    "    compare_loss(country_val_datasets[i], f'country_val_{dataset_config[i]}', path)\n",
    "\n",
    "path = save_path + f\"/country_val/dataset_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "compare_datasets_for_l1(country_val_datasets, 'country_val', path)\n",
    "\n",
    "path = save_path + f\"/country_test/loss_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "for i in range(len(country_test_datasets)):\n",
    "    compare_loss(country_test_datasets[i], f'country_test_{dataset_config[i]}', path)\n",
    "\n",
    "path = save_path + f\"/country_test/dataset_comparison/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "compare_datasets_for_l1(country_test_datasets, 'country_test', path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
