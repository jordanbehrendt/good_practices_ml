{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_repeated_kFold_cv_test(data1, data2, n1, n2, alpha):\n",
    "    \"\"\"\n",
    "    Perform corrected repeated k-fold cross-validation test to evaluate the replicability \n",
    "    of significance tests for comparing learning algorithms.\n",
    "    This implments the test as suggested in the paper \n",
    "    \"A corrected repeated k-fold cross-validation test for replicability in psychophysiology\"\n",
    "    by Bouckaert et al. (2004).\n",
    "\n",
    "    Parameters:\n",
    "    data1 (array-like): The first dataset.\n",
    "    data2 (array-like): The second dataset.\n",
    "    n1 (int): The number of training samples in each fold.\n",
    "    n2 (int): The number of test samples ind each fold.\n",
    "    alpha (float): The significance level.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the degrees of freedom, t-statistic, critical value and the p-value.\n",
    "    \"\"\"\n",
    "    n = len(data1)\n",
    "    if n != len(data2):\n",
    "        raise ValueError(\"The datasets must have the same length.\")\n",
    "    # estimate the mean\n",
    "    m = 1 / n * sum([data1[i] - data2[i] for i in range(n)])\n",
    "    # estimate the standard deviation\n",
    "    stdv_sq = np.sqrt(\n",
    "        1 / (n - 1) * sum([(data1[i] - data2[i] - m) ** 2 for i in range(n)])\n",
    "    )\n",
    "    # calculate the test statistic\n",
    "    t = m / np.sqrt((1 / n + n2 / n1) * stdv_sq)\n",
    "    # degrees of freedom\n",
    "    df = n - 1\n",
    "    # calculate the critical value\n",
    "    cv = stats.t.ppf(1.0 - alpha, df)\n",
    "    # calculate the p-value\n",
    "    p = (1.0 - stats.t.cdf(abs(t), df)) * 2.0\n",
    "    return df, t, cv, p\n",
    "\n",
    "\n",
    "def box_plot_experiments(list_of_df, name, save_path, loss_number=0, dataset_names=None, metric_names=None):\n",
    "    \"\"\"\n",
    "    Genreates box plots for all metrics contained in the dataframes.\n",
    "    Compares these metrics for each dataframe in the list.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_df (list): A list of dataframes.\n",
    "    name (str): The name of the experiment.\n",
    "    save_path (str): The path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated dataframe containing all data with a coloumn tagging the used Loss.\n",
    "    \"\"\"\n",
    "    dataset_to_indices = {'geo_strongly_balanced':0, 'geo_unbalanced':1, 'geo_weakly_balanced':2, 'mixed_strongly_balanced':3, 'mixed_weakly_balanced':4}\n",
    "    if dataset_names is not None:\n",
    "        indices = [dataset_to_indices[name] for name in dataset_names]\n",
    "        list_of_df = [list_of_df[i] for i in indices]\n",
    "\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    list_of_df = [df[loss_number].copy() for df in list_of_df]\n",
    "\n",
    "    cols_to_use = metric_names.copy()\n",
    "    cols_to_use.append('Experiment')\n",
    "\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=list(dataset_to_indices.keys())[indices[i]])\n",
    "        list_of_df[i] = list_of_df[i][cols_to_use]\n",
    "        #cols_to_drop = list_of_df[i].filter(like='text', axis=1).columns\n",
    "        #list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "\n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if type(x) == list else x) \n",
    "    ax = sns.boxplot(\n",
    "        x=\"Metric\", y=\"Value\", hue=\"Experiment\", data=meltdf, showfliers=False\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    lgd = plt.legend(loc='upper left', bbox_to_anchor=(1,1), fontsize='small', borderaxespad=0.0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.savefig(\n",
    "        save_path + f\"{name}-boxplot.png\",\n",
    "        bbox_extra_artists=(lgd,),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return condf\n",
    "\n",
    "\n",
    "def read_event_for_different_seeds(log_dir):\n",
    "    \"\"\"\n",
    "    Read event files for different seeds and extract validation and test data.\n",
    "\n",
    "    Parameters:\n",
    "        log_dir (str): The directory containing the event files of all seeds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four pandas DataFrames:\n",
    "            - region_columns_val: DataFrame containing validation metrics for region columns.\n",
    "            - country_columns_val: DataFrame containing validation metrics for non-region columns.\n",
    "            - region_columns_test: DataFrame containing test metrics for region columns.\n",
    "            - country_columns_test: DataFrame containing test metrics for non-region columns.\n",
    "            - other_columns: DataFrame containing other \n",
    "    \"\"\"\n",
    "    # Get a list of file paths that match the pattern in log_dir\n",
    "    log_files = glob.glob(log_dir + \"/*\")\n",
    "    validation_columns = pd.DataFrame([])\n",
    "    test_columns = pd.DataFrame([])\n",
    "    other_columns = pd.DataFrame([])\n",
    "    # Iterate over the file paths to read each event file\n",
    "    for file_path in log_files:\n",
    "        if '.csv' in file_path: continue\n",
    "        # create a summary iterator for evenet file\n",
    "        sm_iter = summary_iterator(file_path)\n",
    "        # create a buffer to store the validation and test data\n",
    "        validation_buffer = {}\n",
    "        test_buffer = {}\n",
    "        other_buffer = {}\n",
    "        # Iterate over the event file values\n",
    "        for e in sm_iter:\n",
    "            for v in e.summary.value:\n",
    "                tag = v.tag\n",
    "                # filter tags that saved images or matrices\n",
    "                if \"Metrics\" not in tag and \"Matrix\" not in tag:\n",
    "                    # save loss and number of ignored classes/regions\n",
    "                    if 'Loss' in tag:\n",
    "                        if tag not in validation_buffer.keys():\n",
    "                            other_buffer[tag] = []\n",
    "                        other_buffer[tag].append([v.simple_value])\n",
    "                    else:\n",
    "                        if \"Validation\" in tag:\n",
    "                            if tag not in validation_buffer.keys():\n",
    "                                validation_buffer[tag] = []\n",
    "                            if  \"text_summary\" in tag:\n",
    "                                validation_buffer[tag].append([v.tensor.string_val])\n",
    "                            else:\n",
    "                                validation_buffer[tag].append([v.simple_value])\n",
    "                        elif \"Test\" in tag:\n",
    "                            if tag not in test_buffer.keys():\n",
    "                                test_buffer[tag] = []\n",
    "                            if  \"text_summary\" in tag:\n",
    "                                test_buffer[tag].append([v.tensor.string_val])\n",
    "                            else:\n",
    "                                test_buffer[tag].append([v.simple_value])\n",
    "        # Add the Values of the Last epoch to the validation data for each seed \n",
    "        # (validation has 10 folds, test data only 1 value)\n",
    "        validation_columns = pd.concat(\n",
    "            [\n",
    "                validation_columns,\n",
    "                pd.DataFrame(\n",
    "                    {key: values[-10:] for key, values in validation_buffer.items()}\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        test_columns = pd.concat(\n",
    "            [\n",
    "                test_columns,\n",
    "                pd.DataFrame({key: values[-1:] for key, values in test_buffer.items()}),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        other_columns = pd.concat(\n",
    "            [\n",
    "                other_columns,\n",
    "                pd.DataFrame({key: values[-1:] for key, values in other_buffer.items()}),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # Split validation_columns into region_columns and other_columns\n",
    "    region_columns_val = validation_columns.filter(regex=\"Region\")\n",
    "    country_columns_val = validation_columns.loc[\n",
    "        :, ~validation_columns.columns.str.contains(\"Region\")\n",
    "    ]\n",
    "\n",
    "    # Split test_columns into region_columns and other_columns\n",
    "    region_columns_test = test_columns.filter(regex=\"Region\")\n",
    "    country_columns_test = test_columns.loc[\n",
    "        :, ~test_columns.columns.str.contains(\"Region\")\n",
    "    ]\n",
    "    return (\n",
    "        region_columns_val,\n",
    "        country_columns_val,\n",
    "        region_columns_test,\n",
    "        country_columns_test,\n",
    "        other_columns\n",
    "    )\n",
    "\n",
    "def event_to_df(log_dir):\n",
    "    \"\"\"\n",
    "    Converts and merges the event files of multiple seeds into a DataFrame for all directories.\n",
    "    The log_dir should be comtaim multiple folders (e.g. diffrent loss configurations) \n",
    "    that each contain event files for all the used random seeds.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): The directory path containing the event log folders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing lists of dataframes for different columns.\n",
    "            The tuple contains the following lists:\n",
    "            - region_columns_val_list: List of dataframes for region metrics in validation set.\n",
    "            - country_columns_val_list: List of dataframes for country columns in validation set.\n",
    "            - region_columns_test_list: List of dataframes for region columns in test set.\n",
    "            - country_columns_test_list: List of dataframes for country columns in test set.\n",
    "            - other_coloumns_list: List of dataframes for other columns.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the dataframes\n",
    "    region_columns_val_list = []\n",
    "    country_columns_val_list = []\n",
    "    region_columns_test_list = []\n",
    "    country_columns_test_list = []\n",
    "    other_coloumns_list = []\n",
    "\n",
    "    # Iterate over the folders in the log directory\n",
    "    for folder in sorted(os.listdir(log_dir)):\n",
    "        folder_path = os.path.join(log_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Call the read_event_for_different_seeds function for each folder\n",
    "            region_columns_val, coutnry_columns_val, region_columns_test, coutnry_columns_test, other_coloumns = read_event_for_different_seeds(folder_path)\n",
    "            \n",
    "            # Append the dataframes to the respective lists\n",
    "            region_columns_val_list.append(region_columns_val)\n",
    "            country_columns_val_list.append(coutnry_columns_val)\n",
    "            region_columns_test_list.append(region_columns_test)\n",
    "            country_columns_test_list.append(coutnry_columns_test)\n",
    "            other_coloumns_list.append(other_coloumns)\n",
    "\n",
    "    return region_columns_val_list, country_columns_val_list, region_columns_test_list, country_columns_test_list, other_coloumns_list\n",
    "\n",
    "def read_experiment_data(experiment_dir):\n",
    "    # directory of all experiments\n",
    "    # create lists that contain the dataframes of the different experiments\n",
    "    # First axis contains the different dataset configurations\n",
    "    # Second axis contains the different Loss configurations\n",
    "    # Third axis contains the DataFrame of the different seeds\n",
    "    region_val_datasets = []\n",
    "    country_val_datasets = []\n",
    "    region_test_datasets = []\n",
    "    country_test_datasets = []\n",
    "    other_coloumns_list = []\n",
    "\n",
    "    for folder in sorted(os.listdir(experiment_dir)):\n",
    "        log_dir = os.path.join(experiment_dir, folder)\n",
    "        if os.path.isdir(log_dir):\n",
    "            if 'balanced' not in log_dir:\n",
    "                continue\n",
    "            save_path = log_dir + '/results/'\n",
    "            # Call the event_to_df function with the log directory \n",
    "            rv, cv, rt, ct, o = event_to_df(log_dir)\n",
    "            region_val_datasets.append(rv)\n",
    "            country_val_datasets.append(cv)\n",
    "            region_test_datasets.append(rt)\n",
    "            country_test_datasets.append(ct)\n",
    "            other_coloumns_list.append(o)\n",
    "    return region_val_datasets, country_val_datasets, region_test_datasets, country_test_datasets, other_coloumns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'path/to/experiment/logs/'\n",
    "save_path = 'path/to/save/plots/'\n",
    "region_columns_val_list, coutnry_columns_val_list, region_columns_test_list, country_columns_test_list, other_coloumns_list = read_experiment_data(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['geo_strongly_balanced', 'geo_weakly_balanced', 'mixed_strongly_balanced', 'mixed_weakly_balanced']\n",
    "val_region_metric_names = ['Epoch Validation avg Region F1', 'Epoch Validation avg Region Precision', 'Epoch Validation avg Region Recall', 'Validation Regional Accuracy']\n",
    "val_country_metric_names = ['Epoch Validation avg Class F1', 'Epoch Validation avg Class Precision', 'Epoch Validation avg Class Recall', 'Validation Accuracy']\n",
    "test_region_metric_names = ['Test avg Region F1', 'Test avg Region Precision', 'Test avg Region Recall', 'Test Regional Accuracy']\n",
    "test_country_metric_names = ['Test avg Class F1', 'Test avg Class Precision', 'Test avg Class Recall', 'Test Accuracy']\n",
    "# Call the event_to_df function with the log directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the box_plot_experiments function with the lists of dataframes\n",
    "val_region_metrics = box_plot_experiments(region_columns_val_list, 'Validation Set - Region', save_path, dataset_names=dataset_names, metric_names=val_region_metric_names)   \n",
    "val_country_metrics = box_plot_experiments(coutnry_columns_val_list, 'Validation Set - Country', save_path, dataset_names=dataset_names, metric_names=val_country_metric_names)\n",
    "test_region_metric = box_plot_experiments(region_columns_test_list, 'Test Set - Region', save_path, dataset_names=dataset_names, metric_names=test_region_metric_names)\n",
    "test_coutnry_metric = box_plot_experiments(country_columns_test_list, 'Test Set - Country', save_path, dataset_names=dataset_names, metric_names=test_country_metric_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gpml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
